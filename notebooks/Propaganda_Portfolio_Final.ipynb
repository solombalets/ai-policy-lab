{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bc0ade4",
   "metadata": {},
   "source": [
    "## üì¶ Environment & Dependencies\n",
    "\n",
    "This section imports all the necessary libraries. Make sure to install missing ones via `pip install package-name` if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fd14ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "import emoji\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import umap\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from collections import Counter\n",
    "\n",
    "tqdm.pandas()\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe2f1e8",
   "metadata": {},
   "source": [
    "## üìä Load and Prepare Data\n",
    "\n",
    "This section loads the merged and clustered dataset and performs basic preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b72c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed and clustered data\n",
    "df = pd.read_csv(\"embedded_clustered.csv\")\n",
    "df = df.dropna(subset=[\"text\"])\n",
    "\n",
    "# Check the structure\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5796bca",
   "metadata": {},
   "source": [
    "## üåê UMAP Cluster Visualization\n",
    "\n",
    "We visualize the clustered embeddings using UMAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d14cf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(\n",
    "    x=\"umap_x\", y=\"umap_y\", hue=\"cluster\", data=df,\n",
    "    palette=\"tab10\", legend=\"full\", alpha=0.7\n",
    ")\n",
    "plt.title(\"UMAP Projection of Text Clusters\")\n",
    "plt.xlabel(\"UMAP 1\")\n",
    "plt.ylabel(\"UMAP 2\")\n",
    "plt.legend(title=\"Cluster\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fcd051",
   "metadata": {},
   "source": [
    "## ‚òÅÔ∏è WordClouds for Key Concepts\n",
    "\n",
    "We visualize semantic associations around selected terms like '—Ä—É—Å—Å–∫–∏–π', '–Ω–∞—Ä–æ–¥', and '–∑–∞–ø–∞–¥'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0699df92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context_wordcloud(df, keyword, window=10):\n",
    "    texts = df[\"text\"].dropna().astype(str).tolist()\n",
    "    tokens = [nltk.word_tokenize(text.lower()) for text in texts]\n",
    "    context_words = []\n",
    "\n",
    "    for sentence in tokens:\n",
    "        for i, word in enumerate(sentence):\n",
    "            if keyword in word:\n",
    "                left = sentence[max(0, i - window):i]\n",
    "                right = sentence[i+1:i + 1 + window]\n",
    "                context_words.extend(left + right)\n",
    "\n",
    "    freq = Counter(context_words)\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(freq)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"WordCloud for context around '{keyword}'\")\n",
    "    plt.show()\n",
    "\n",
    "generate_context_wordcloud(df, \"—Ä—É—Å—Å–∫\")\n",
    "generate_context_wordcloud(df, \"–Ω–∞—Ä–æ–¥\")\n",
    "generate_context_wordcloud(df, \"–∑–∞–ø–∞–¥\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0c7446",
   "metadata": {},
   "source": [
    "## üéØ Extracting Dominant Terms by Cluster\n",
    "\n",
    "TF-IDF is used to identify potentially manipulative or propagandistic lexical fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e499cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "\n",
    "clusters = df[\"cluster\"].unique()\n",
    "top_n = 15\n",
    "tfidf_results = {}\n",
    "\n",
    "for cluster in clusters:\n",
    "    texts = df[df[\"cluster\"] == cluster][\"text\"].astype(str).tolist()\n",
    "    vectorizer = TfidfVectorizer(max_df=0.9, min_df=2, stop_words=russian_stopwords, max_features=1000)\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    tfidf_scores = tfidf_matrix.sum(axis=0).A1\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "    top_indices = tfidf_scores.argsort()[::-1][:top_n]\n",
    "    top_terms = [(terms[i], tfidf_scores[i]) for i in top_indices]\n",
    "    tfidf_results[cluster] = top_terms\n",
    "\n",
    "for cluster, terms in tfidf_results.items():\n",
    "    print(f\"\\nCluster {cluster}:\")\n",
    "    for term, score in terms:\n",
    "        print(f\"{term} ({score:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4502dd79",
   "metadata": {},
   "source": [
    "## üßæ Summary\n",
    "\n",
    "This notebook demonstrates how to use NLP and clustering techniques to explore rhetorical and semantic patterns in Russian pro-government Telegram discourse. The result highlights the thematic fragmentation, key identity symbols, and potential propagandistic markers across different narrative clusters."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
