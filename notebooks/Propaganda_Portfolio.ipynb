{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af43e38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source information: Telegram channels used\n",
    "channels = [\n",
    "    \"Agdchan\",\n",
    "    \"readovkanews\",\n",
    "    \"medvedev_telegram\",\n",
    "    \"warfakes\",\n",
    "    \"MariaVladimirovnaZakharova\",\n",
    "    \"alexey_pushkov\",\n",
    "    \"RKadyrov_95\",\n",
    "    \"holmogor_talks\",\n",
    "    \"dva_majors\",\n",
    "    \"grey_zone\",\n",
    "    \"philologist_zov\"\n",
    "]\n",
    "print(\"Data was collected from the following Telegram channels:\")\n",
    "for ch in channels:\n",
    "    print(f\" - @{ch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeceedc40ab03a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T12:39:48.801298Z",
     "start_time": "2025-05-10T12:39:47.418237Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "russian_stopwords = stopwords.words(\"russian\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c3b2e5cd38d09e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T14:46:49.266168Z",
     "start_time": "2025-05-12T14:46:49.252080Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16a0107556b3eb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T15:44:42.854586Z",
     "start_time": "2025-05-12T15:44:42.822948Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "print(nltk.data.find(\"tokenizers/punkt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdcae79ec81dbc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T15:50:53.263034Z",
     "start_time": "2025-05-12T15:50:53.251225Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.data.path.append(r\"C:\\Users\\yuter\\AppData\\Roaming\\nltk_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb493652bf0b47a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T15:56:56.514377Z",
     "start_time": "2025-05-12T15:56:56.500576Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bdc99c1d97d690",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T16:01:55.503467Z",
     "start_time": "2025-05-12T16:01:55.489617Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54ff2354b510033",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T16:05:10.719032Z",
     "start_time": "2025-05-12T16:04:31.617312Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d73034dfb13e65e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T16:05:23.310230Z",
     "start_time": "2025-05-12T16:05:23.284428Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "print(nltk.data.find(\"tokenizers/punkt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa2688f02174f33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T16:08:47.448710Z",
     "start_time": "2025-05-12T16:08:47.388250Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "print(\"‚úÖ –í—Å–µ —Ä–µ—Å—É—Ä—Å—ã NLTK –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e138bfef4534d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T16:17:27.397504Z",
     "start_time": "2025-05-12T16:17:23.232743Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"ru_core_news_md\")\n",
    "print(\"‚úÖ –†—É—Å—Å–∫–∞—è –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7892f07878505b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T16:18:22.076674Z",
     "start_time": "2025-05-12T16:18:18.990058Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nlp = spacy.load(\"ru_core_news_md\")\n",
    "stop_words = set(stopwords.words(\"russian\"))\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç: —É–¥–∞–ª—è–µ—Ç –ª–∏—à–Ω–µ–µ –∏ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º–µ.\"\"\"\n",
    "    text = text.lower()  # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É\n",
    "    text = re.sub(r\"http\\S+|www\\S+|@\\w+|\\d+\", \"\", text)  # –£–¥–∞–ª—è–µ–º —Å—Å—ã–ª–∫–∏, —É–ø–æ–º–∏–Ω–∞–Ω–∏—è, —Ü–∏—Ñ—Ä—ã\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # –£–¥–∞–ª—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é\n",
    "\n",
    "    doc = nlp(text)  # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç –≤ spacy\n",
    "\n",
    "    words = [token.lemma_ for token in doc if token.text not in stop_words and token.pos_ in {\"NOUN\", \"ADJ\"}]  # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è + —Ñ–∏–ª—å—Ç—Ä\n",
    "    return \" \".join(words)\n",
    "\n",
    "# –¢–µ—Å—Ç:\n",
    "example_text = \"‚ö°Ô∏è–í —Å—Ä–æ—á–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ! –í–°–£ –∞—Ç–∞–∫–æ–≤–∞–ª–∏ –º–∏—Ä–Ω—ã—Ö –∂–∏—Ç–µ–ª–µ–π –≤ –î–æ–Ω–µ—Ü–∫–µ! üì¢ –°–º–æ—Ç—Ä–∏—Ç–µ –≤–∏–¥–µ–æ –∑–¥–µ—Å—å: https://t.me/example\"\n",
    "print(clean_text(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf531c495116796c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T16:38:47.757752Z",
     "start_time": "2025-05-12T16:18:54.328694Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"bertopic_clustered_results.csv\")\n",
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
    "df.to_csv(\"cleaned_bertopic_results.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: cleaned_bertopic_results.csv!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b8b80c12a807b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T13:30:45.018520Z",
     "start_time": "2025-05-13T13:30:42.061382Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model_name = \"QCRI/PropagandaTechniquesAnalysis-en-BERT\"\n",
    "try:\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    print(\"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå –û—à–∏–±–∫–∞: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f574879564c68e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T13:42:51.943846Z",
     "start_time": "2025-05-13T13:42:48.042534Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"facebook/bart-large-mnli\"  # ‚Üê –ü–æ–ø—Ä–æ–±—É–π –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(\"‚úÖ Tokenizer –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6356e2ae41389d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T13:43:08.621832Z",
     "start_time": "2025-05-13T13:43:07.597923Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-classification\", model=\"DeepPavlov/rubert-base-cased\")  # –†—É—Å—Å–∫–∏–π BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0077593177c109d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T11:55:56.416648Z",
     "start_time": "2025-05-14T11:55:56.406613Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GIT_PYTHON_GIT_EXECUTABLE\"] = \"C:\\\\Program Files\\\\Git\\\\bin\\\\git.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600a9b5d9dfc5682",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T12:18:55.135767Z",
     "start_time": "2025-05-15T12:18:55.124766Z"
    }
   },
   "outputs": [],
   "source": [
    "print(dataset)\n",
    "print(dataset.column_names)\n",
    "print(dataset[\"train\"][0])  # –í—ã–≤–µ–¥–µ–º –ø–µ—Ä–≤—ã–π –ø—Ä–∏–º–µ—Ä –∏–∑ train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1367fe4be2ec3dbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T14:19:15.329215Z",
     "start_time": "2025-05-15T13:02:08.525664Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# üìå 1. –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –º–æ–¥–µ–ª—å\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
    "\n",
    "# üìå 2. –°–æ–∑–¥–∞—ë–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –º–µ—Ç–æ–∫ –∏ —Å–ª–æ–≤–∞—Ä—å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è\n",
    "label_list = sorted(set(label for labels in dataset[\"train\"][\"labels\"] for label in (labels if isinstance(labels, list) else [labels])))\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}  # –°–æ–∑–¥–∞—ë–º —Å–ª–æ–≤–∞—Ä—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π\n",
    "\n",
    "# üìå 3. –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å —É—á—ë—Ç–æ–º –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"DeepPavlov/rubert-base-cased\",\n",
    "    num_labels=len(label_list)  # –£–∫–∞–∑—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤\n",
    ")\n",
    "\n",
    "# üìå 4. –§—É–Ω–∫—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ + –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –º–µ—Ç–æ–∫ –≤ –∏–Ω–¥–µ–∫—Å—ã\n",
    "def preprocess_function(example):\n",
    "    return {\n",
    "        **tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=512),\n",
    "        \"labels\": [label_to_id[label] for label in example[\"labels\"]]  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –º–µ—Ç–∫–∏ –≤ –∏–Ω–¥–µ–∫—Å—ã\n",
    "    }\n",
    "\n",
    "# üìå 5. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç\n",
    "train_dataset = dataset[\"train\"].map(preprocess_function, batched=True)\n",
    "test_dataset = dataset[\"test\"].map(preprocess_function, batched=True)\n",
    "\n",
    "# üìå 6. –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fallacy_bert\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# üìå 7. –ó–∞–ø—É—Å–∫ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13712930f79c0b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T14:20:57.586879Z",
     "start_time": "2025-05-15T14:20:11.843558Z"
    }
   },
   "outputs": [],
   "source": [
    "metrics = trainer.evaluate()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22ce6d5b6d7578a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T14:23:11.991705Z",
     "start_time": "2025-05-15T14:23:11.279309Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"./fallacy_bert\")  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8279b4b89fa33ca9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T14:24:51.867764Z",
     "start_time": "2025-05-15T14:24:51.854319Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(os.listdir(\"./fallacy_bert\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24386f93d4173a69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T14:27:19.148889Z",
     "start_time": "2025-05-15T14:27:05.437956Z"
    }
   },
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "for proc in psutil.process_iter(['pid', 'name']):\n",
    "    try:\n",
    "        for item in proc.open_files():\n",
    "            if './fallacy_bert' in item.path:\n",
    "                print(f\"–ü—Ä–æ—Ü–µ—Å—Å {proc.info['name']} (PID: {proc.info['pid']}) –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ñ–∞–π–ª {item.path}\")\n",
    "    except psutil.AccessDenied:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3ffbb56ea927e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T14:43:57.731268Z",
     "start_time": "2025-05-15T14:43:57.689269Z"
    }
   },
   "outputs": [],
   "source": [
    "label_list = sorted(set(label for example in dataset[\"train\"] for label in (example[\"labels\"] if isinstance(example[\"labels\"], list) else [example[\"labels\"]])))\n",
    "id_to_label = {i: label for i, label in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f70eb95c1491ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T14:44:16.434433Z",
     "start_time": "2025-05-15T14:44:16.421425Z"
    }
   },
   "outputs": [],
   "source": [
    "decoded_predictions = [\n",
    "    {\"label\": id_to_label[int(pred[\"label\"].split(\"_\")[-1])], \"score\": pred[\"score\"]}\n",
    "    for pred in predictions\n",
    "]\n",
    "\n",
    "print(decoded_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb78ad36df55e64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T15:51:10.733146Z",
     "start_time": "2025-05-15T15:51:10.715537Z"
    }
   },
   "outputs": [],
   "source": [
    "label_list = list(set(dataset[\"train\"][\"labels\"]))  # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –º–µ—Ç–∫–∏\n",
    "id_to_label = {i: label for i, label in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf1efa0ad791103",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T15:51:19.766018Z",
     "start_time": "2025-05-15T15:51:19.757018Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"üîπ –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏:\", label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab98c2241c2826b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T15:55:24.599559Z",
     "start_time": "2025-05-15T15:55:24.589977Z"
    }
   },
   "outputs": [],
   "source": [
    "train_labels = dataset[\"train\"][\"labels\"]  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–µ—Å—å —Å–ø–∏—Å–æ–∫ –º–µ—Ç–æ–∫\n",
    "label_distribution = Counter([label for sublist in train_labels for label in sublist])  # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5a89faed0a4e1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T15:55:32.866066Z",
     "start_time": "2025-05-15T15:55:32.858067Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"üîπ –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö:\", train_labels[:5])  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 –∑–∞–ø–∏—Å–µ–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48df5a44851b8dc9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T15:56:11.097487Z",
     "start_time": "2025-05-15T15:56:11.086486Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# –°–æ–∑–¥–∞—ë–º —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫\n",
    "predicted_labels = [pred[\"label\"] for pred in decoded_predictions]\n",
    "\n",
    "# –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏ –∫–∞–∂–¥–æ–π –æ—à–∏–±–∫–∏\n",
    "fallacy_counts = Counter(predicted_labels)\n",
    "\n",
    "print(\"üîπ –ß–∞—Å—Ç–æ—Ç–∞ –æ—à–∏–±–æ–∫:\", fallacy_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6643c3769df811a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T15:57:09.861280Z",
     "start_time": "2025-05-15T15:57:09.850280Z"
    }
   },
   "outputs": [],
   "source": [
    "print(predictions[:10])  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 10 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882e0708a6be47e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T15:57:31.826324Z",
     "start_time": "2025-05-15T15:57:31.804325Z"
    }
   },
   "outputs": [],
   "source": [
    "decoded_predictions = [\n",
    "    {\"label\": id_to_label.get(int(pred[\"label\"].split(\"_\")[-1]), \"UNKNOWN\"), \"score\": pred[\"score\"]}\n",
    "    for pred in predictions\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4cfaa3ab5ed85f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T15:57:40.635468Z",
     "start_time": "2025-05-15T15:57:40.618469Z"
    }
   },
   "outputs": [],
   "source": [
    "fallacy_counts = Counter([pred[\"label\"] for pred in decoded_predictions])\n",
    "print(\"üîπ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –æ—à–∏–±–æ–∫:\", fallacy_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775cc25b61b50f67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T15:59:29.715531Z",
     "start_time": "2025-05-15T15:59:29.696180Z"
    }
   },
   "outputs": [],
   "source": [
    "predicted_labels = [pred[\"label\"] for pred in decoded_predictions]\n",
    "fallacy_counts = Counter(predicted_labels)\n",
    "\n",
    "print(\"üîπ –ß–∞—Å—Ç–æ—Ç–∞ –æ—à–∏–±–æ–∫ –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è—Ö:\", fallacy_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf1d306497bb506",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T15:59:48.357646Z",
     "start_time": "2025-05-15T15:59:48.341495Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "train_labels = [label for example in dataset[\"train\"][\"labels\"] for label in example]\n",
    "label_distribution = Counter(train_labels)\n",
    "\n",
    "print(\"üîπ –ß–∞—Å—Ç–æ—Ç–∞ –æ—à–∏–±–æ–∫ –≤ –æ–±—É—á–∞—é—â–µ–º –¥–∞—Ç–∞—Å–µ—Ç–µ:\", label_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4a10dd02aef265",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:00:41.112594Z",
     "start_time": "2025-05-15T16:00:41.082141Z"
    }
   },
   "outputs": [],
   "source": [
    "label_list = sorted(set(label for example in dataset[\"train\"] for label in (example[\"labels\"] if isinstance(example[\"labels\"], list) else [example[\"labels\"]])))\n",
    "id_to_label = {i: label for i, label in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a068afd7692b8d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:00:48.683729Z",
     "start_time": "2025-05-15T16:00:48.661991Z"
    }
   },
   "outputs": [],
   "source": [
    "fallacy_counts = Counter([pred[\"label\"] for pred in decoded_predictions])\n",
    "print(\"üîπ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫:\", fallacy_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ff44897cad7266",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:02:30.880583Z",
     "start_time": "2025-05-15T16:02:30.874578Z"
    }
   },
   "outputs": [],
   "source": [
    "print([pred[\"label\"] for pred in predictions[:20]])  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 20 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d2b97fff33c4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:02:45.234232Z",
     "start_time": "2025-05-15T16:02:45.213248Z"
    }
   },
   "outputs": [],
   "source": [
    "print(decoded_predictions[:20])  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 20 –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cafa9aaf68402c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:03:11.509557Z",
     "start_time": "2025-05-15T16:03:04.610897Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./fallacy_bert\")\n",
    "token_lengths = [len(tokenizer.tokenize(text)) for text in texts]\n",
    "\n",
    "print(f\"üîπ –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤: {sum(token_lengths) / len(token_lengths):.2f}\")\n",
    "print(f\"üîπ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {max(token_lengths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb48891b9dc52f09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:04:51.593078Z",
     "start_time": "2025-05-15T16:04:51.579053Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"üîπ –í—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –º–µ—Ç–∫–∏:\", id_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2143f0800fa27cac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:06:40.321591Z",
     "start_time": "2025-05-15T16:06:40.306591Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"üîπ –†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞:\", len(dataset[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca137fd51f8446a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:07:33.304538Z",
     "start_time": "2025-05-15T16:07:33.293537Z"
    }
   },
   "outputs": [],
   "source": [
    "scores = [pred[\"score\"] for pred in decoded_predictions]\n",
    "print(f\"üîπ –°—Ä–µ–¥–Ω–∏–π score: {sum(scores) / len(scores):.4f}\")\n",
    "print(f\"üîπ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π score: {max(scores)}\")\n",
    "print(f\"üîπ –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π score: {min(scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50f8bb0a21afc6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:11:45.149627Z",
     "start_time": "2025-05-15T16:10:37.540911Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_name = \"bert-base-multilingual-cased\"  # –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é BERT-–º–æ–¥–µ–ª—å\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_list))\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3382fa565cfbc08d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:12:53.860891Z",
     "start_time": "2025-05-15T16:12:53.586753Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"]  # –ü—Ä–æ—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º train –∏–∑ dataset\n",
    "train_dataset = train_dataset.map(tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c39c43f52cee4a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:13:04.464333Z",
     "start_time": "2025-05-15T16:13:04.449173Z"
    }
   },
   "outputs": [],
   "source": [
    "print(type(dataset[\"train\"]))  # –î–æ–ª–∂–Ω–æ –≤—ã–≤–µ—Å—Ç–∏ <class 'datasets.Dataset'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e2e21eb6fd5291",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:17:32.266280Z",
     "start_time": "2025-05-15T16:17:32.238829Z"
    }
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_finetuned\",\n",
    "    eval_strategy=\"no\",  # –û—Ç–∫–ª—é—á–∞–µ–º eval_strategy, –µ—Å–ª–∏ –Ω–µ—Ç eval_dataset\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c245864d6b7f0a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:19:56.550839Z",
     "start_time": "2025-05-15T16:19:56.222477Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "train_dataset = train_dataset.map(tokenize_function)\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –º–µ—Ç–∫–∏\n",
    "print(train_dataset.column_names)  # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å \"labels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d815915faea6a1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:21:18.270819Z",
     "start_time": "2025-05-15T16:21:18.228549Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f86226652ca24af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:23:27.917568Z",
     "start_time": "2025-05-15T16:23:27.897018Z"
    }
   },
   "outputs": [],
   "source": [
    "for key in [\"input_ids\", \"attention_mask\", \"labels\"]:\n",
    "    print(f\"üîπ {key}: {type(batch[key])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3499f07f100811",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:25:56.810336Z",
     "start_time": "2025-05-15T16:25:56.716337Z"
    }
   },
   "outputs": [],
   "source": [
    "label_to_id = {v: k for k, v in id_to_label.items()}  # –ü–µ—Ä–µ–≤–æ—Ä–∞—á–∏–≤–∞–µ–º —Å–ª–æ–≤–∞—Ä—å\n",
    "train_dataset = train_dataset.map(lambda example: {\"labels\": label_to_id[example[\"labels\"]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9c05342865c1b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:26:05.694052Z",
     "start_time": "2025-05-15T16:26:05.676053Z"
    }
   },
   "outputs": [],
   "source": [
    "print(train_dataset[0][\"labels\"])  # –î–æ–ª–∂–Ω–æ –≤—ã–≤–µ—Å—Ç–∏ —á–∏—Å–ª–æ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 3, 7, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311c75ac3d113f8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:28:07.211833Z",
     "start_time": "2025-05-15T16:28:07.195834Z"
    }
   },
   "outputs": [],
   "source": [
    "print(model.config)  # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å —á—Ç–æ-—Ç–æ –≤—Ä–æ–¥–µ `num_labels=13`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a78cb7bddb115a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:29:40.854552Z",
     "start_time": "2025-05-15T16:29:40.479863Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-cased\",\n",
    "    num_labels=len(label_list)  # –ó–∞–¥–∞—ë–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ –æ—à–∏–±–æ–∫\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f16210b4f2905a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:29:51.948171Z",
     "start_time": "2025-05-15T16:29:51.936177Z"
    }
   },
   "outputs": [],
   "source": [
    "print(model.config.num_labels)  # –î–æ–ª–∂–Ω–æ –≤—ã–≤–µ—Å—Ç–∏ `13` (–∏–ª–∏ –¥—Ä—É–≥–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424ca81d94f43190",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:31:26.374487Z",
     "start_time": "2025-05-15T16:31:26.095483Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-cased\",\n",
    "    num_labels=len(label_list)  # –£–∫–∞–∑—ã–≤–∞–µ–º —á–∏—Å–ª–æ –∫–ª–∞—Å—Å–æ–≤\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a1e0f8d62e97c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:34:10.222878Z",
     "start_time": "2025-05-15T16:34:10.156282Z"
    }
   },
   "outputs": [],
   "source": [
    "label_to_id = {v: k for k, v in id_to_label.items()}  # –ü–µ—Ä–µ–≤–æ—Ä–∞—á–∏–≤–∞–µ–º —Å–ª–æ–≤–∞—Ä—å\n",
    "train_dataset = train_dataset.map(lambda example: {\"labels\": label_to_id.get(example[\"labels\"], -1)})  # –ò—Å–ø–æ–ª—å–∑—É–µ–º .get(), —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å KeyError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b667ac986d0af3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:35:27.592749Z",
     "start_time": "2025-05-15T16:35:27.292001Z"
    }
   },
   "outputs": [],
   "source": [
    "print(set(example[\"labels\"] for example in train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c186113689cb80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:35:37.939098Z",
     "start_time": "2025-05-15T16:35:37.853241Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(lambda example: {\"labels\": label_to_id.get(example[\"labels\"], \"UNKNOWN\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74aa85f5bb7df6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:37:22.429877Z",
     "start_time": "2025-05-15T16:37:22.408871Z"
    }
   },
   "outputs": [],
   "source": [
    "print(train_dataset.column_names)  # –î–æ–ª–∂–Ω–æ –≤–∫–ª—é—á–∞—Ç—å \"labels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6f03e61a4aa750",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:37:36.056615Z",
     "start_time": "2025-05-15T16:37:35.917577Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(lambda example: {\"labels\": label_to_id.get(example[\"labels\"], -1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cdac4f1cebaa77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:37:56.035816Z",
     "start_time": "2025-05-15T16:37:55.981749Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52739b95c79a2b03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:38:10.984559Z",
     "start_time": "2025-05-15T16:38:10.972560Z"
    }
   },
   "outputs": [],
   "source": [
    "print(model.config)  # –î–æ–ª–∂–Ω–æ –≤–∫–ª—é—á–∞—Ç—å \"num_labels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1623dbce1a7c9148",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:39:39.873304Z",
     "start_time": "2025-05-15T16:39:39.471402Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.filter(lambda example: example[\"labels\"] in label_to_id.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75665fd2b86ab7c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:42:58.257103Z",
     "start_time": "2025-05-15T16:42:58.223386Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"üîπ label_to_id:\", label_to_id)\n",
    "print(\"üîπ –ú–µ—Ç–∫–∏ –≤ train_dataset:\", set(example[\"labels\"] for example in dataset[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da6d6548b7ec8f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:45:03.230523Z",
     "start_time": "2025-05-15T16:45:03.199523Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.filter(lambda example: example[\"labels\"] != -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb05267cb983267",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:45:16.645717Z",
     "start_time": "2025-05-15T16:45:16.635721Z"
    }
   },
   "outputs": [],
   "source": [
    "for example in train_dataset:\n",
    "    if example[\"labels\"] == -1:\n",
    "        print(\"‚ùå –û—à–∏–±–∫–∞: –º–µ—Ç–∫–∞ -1 –Ω–∞–π–¥–µ–Ω–∞!\", example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b22b197eabc8c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:46:35.727351Z",
     "start_time": "2025-05-15T16:46:35.703403Z"
    }
   },
   "outputs": [],
   "source": [
    "count_invalid = sum(1 for example in train_dataset if example[\"labels\"] == -1)\n",
    "print(f\"‚ùå –û—Å—Ç–∞–ª–æ—Å—å {count_invalid} –ø—Ä–∏–º–µ—Ä–æ–≤ —Å -1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ebdf238ff64fbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:47:19.775851Z",
     "start_time": "2025-05-15T16:47:19.737533Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(lambda example: {\"labels\": max(example[\"labels\"], 0)})  # -1 ‚Üí 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbe9cb5637c5657",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:47:27.515195Z",
     "start_time": "2025-05-15T16:47:27.507193Z"
    }
   },
   "outputs": [],
   "source": [
    "print(set(example[\"labels\"] for example in train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130d94426b5678c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:49:14.386766Z",
     "start_time": "2025-05-15T16:49:14.365770Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(lambda example: {\"labels\": label_to_id.get(example[\"labels\"], 0)})  # `-1` ‚Üí `0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b9ab208d313394",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:52:48.028429Z",
     "start_time": "2025-05-15T16:52:47.915871Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"].map(lambda example: {\"labels\": label_to_id.get(example[\"labels\"], 0), **example})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb22a4108da55aab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:59:08.397323Z",
     "start_time": "2025-05-15T16:59:08.389322Z"
    }
   },
   "outputs": [],
   "source": [
    "print(label_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ca4a33b0d1b723",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T17:00:07.220450Z",
     "start_time": "2025-05-15T17:00:07.179418Z"
    }
   },
   "outputs": [],
   "source": [
    "count_invalid = sum(1 for example in train_dataset if example[\"labels\"] == -1)\n",
    "print(f\"‚ùå –û—Å—Ç–∞–ª–æ—Å—å {count_invalid} –ø—Ä–∏–º–µ—Ä–æ–≤ —Å -1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6697e50c2729779c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T17:11:12.402952Z",
     "start_time": "2025-05-15T17:11:12.376406Z"
    }
   },
   "outputs": [],
   "source": [
    "print(label_to_id)  # –î–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å {'ad hominem': 0, 'false dilemma': 1, ...}\n",
    "print(set(example[\"labels\"] for example in train_dataset))  # –î–æ–ª–∂–Ω—ã –±—ã—Ç—å —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–∞ –æ—Ç `0` –¥–æ `num_labels - 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3752081a45f4e9d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T17:06:29.672389Z",
     "start_time": "2025-05-15T17:06:29.641552Z"
    }
   },
   "outputs": [],
   "source": [
    "model.config.num_labels = len(label_to_id)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc1b6610066611",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T17:11:07.824655Z",
     "start_time": "2025-05-15T17:11:07.738652Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(lambda example: {\"labels\": label_to_id[example[\"labels\"]], **example})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7c5faa74aafa36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T17:12:32.382351Z",
     "start_time": "2025-05-15T17:12:32.333334Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_labels(example):\n",
    "    if example[\"labels\"] in label_to_id:\n",
    "        new_label = label_to_id[example[\"labels\"]]\n",
    "        print(f\"‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è: {example['labels']} ‚Üí {new_label}\")  # –í—ã–≤–æ–¥–∏–º –∑–∞–º–µ–Ω—É\n",
    "        return {\"labels\": new_label, **example}\n",
    "    else:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞: {example['labels']} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ `label_to_id`\")\n",
    "        return example  # –û—Å—Ç–∞–≤–ª—è–µ–º –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π, —á—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å, —á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç\n",
    "\n",
    "train_dataset = train_dataset.map(convert_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357ab17783df01a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T17:18:30.923911Z",
     "start_time": "2025-05-15T17:18:30.867873Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "dataset_new = DatasetDict({\n",
    "    \"train\": train_dataset.map(lambda example: {\"labels\": label_to_id[example[\"labels\"]], **example})\n",
    "})\n",
    "\n",
    "print(set(example[\"labels\"] for example in dataset_new[\"train\"]))  # –î–æ–ª–∂–Ω—ã –±—ã—Ç—å —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a745f6847cb5ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T17:24:39.552283Z",
     "start_time": "2025-05-15T17:24:39.464404Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import Value\n",
    "\n",
    "# –®–ê–ì 1: –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π —Å—Ç–æ–ª–±–µ—Ü —Å —á–∏—Å–ª–æ–≤—ã–º–∏ –º–µ—Ç–∫–∞–º–∏.\n",
    "def convert_labels(example):\n",
    "    # –ï—Å–ª–∏ –º–µ—Ç–∫–∞ —Ö—Ä–∞–Ω–∏—Ç—Å—è –≤ —Å–ø–∏—Å–∫–µ, –≤–æ–∑—å–º—ë–º –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç (–∏–ª–∏ –ø—Ä–æ–¥—É–º–∞–π—Ç–µ –ª–æ–≥–∏–∫—É –¥–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –º–µ—Ç–æ–∫)\n",
    "    label_str = example[\"labels\"][0] if isinstance(example[\"labels\"], list) else example[\"labels\"]\n",
    "    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–æ–≤—ã–π —Å—Ç–æ–ª–±–µ—Ü \"int_labels\" —Å —á–∏—Å–ª–æ–≤—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º\n",
    "    return {\"int_labels\": label_to_id[label_str]}\n",
    "\n",
    "# –ü—Ä–∏–º–µ–Ω—è–µ–º map –∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç\n",
    "train_dataset_new = train_dataset.map(convert_labels, batched=False)\n",
    "\n",
    "# –®–ê–ì 2: –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–π —Å—Ç–æ–ª–±–µ—Ü \"labels\" –∏ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º \"int_labels\" –≤ \"labels\"\n",
    "train_dataset_new = train_dataset_new.remove_columns(\"labels\")\n",
    "train_dataset_new = train_dataset_new.rename_column(\"int_labels\", \"labels\")\n",
    "\n",
    "# –®–ê–ì 3: –ü—Ä–∏–≤–æ–¥–∏–º —Å—Ç–æ–ª–±–µ—Ü \"labels\" –∫ —Ç–∏–ø—É int (int64)\n",
    "train_dataset_new = train_dataset_new.cast_column(\"labels\", Value(\"int64\"))\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ —Ç–µ–ø–µ—Ä—å —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–∞\n",
    "print(set(example[\"labels\"] for example in train_dataset_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd725d09aea5c49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T17:35:49.441205Z",
     "start_time": "2025-05-15T17:35:49.306732Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å truncation, padding –∏ max_length –ø–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏.\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –∫–æ –≤—Å–µ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É.\n",
    "tokenized_dataset = train_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b028c6742f1654",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T17:35:52.006232Z",
     "start_time": "2025-05-15T17:35:51.950437Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_labels(example):\n",
    "    # –ï—Å–ª–∏ –º–µ—Ç–∫–∞ –æ–∫–∞–∑–∞–ª–∞—Å—å —Å–ø–∏—Å–∫–æ–º ‚Äì –±–µ—Ä—ë–º –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç\n",
    "    label_str = example[\"labels\"][0] if isinstance(example[\"labels\"], list) else example[\"labels\"]\n",
    "    return {\"int_labels\": label_to_id[label_str]}\n",
    "\n",
    "# –ü—Ä–∏–º–µ–Ω—è–µ–º map –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–æ–≤–æ–≥–æ —á–∏—Å–ª–æ–≤–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞\n",
    "tokenized_dataset = tokenized_dataset.map(convert_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809dc968a61371a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T17:35:54.620717Z",
     "start_time": "2025-05-15T17:35:54.520998Z"
    }
   },
   "outputs": [],
   "source": [
    "# –≠—Ç–æ –¥–æ–ª–∂–Ω–æ –≤—ã–≤–µ—Å—Ç–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è, –Ω–∞–ø—Ä–∏–º–µ—Ä, {0, 1, ...}\n",
    "print(set(example[\"int_labels\"] for example in tokenized_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9a0ada147b04d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T17:36:36.576426Z",
     "start_time": "2025-05-15T17:36:36.558427Z"
    }
   },
   "outputs": [],
   "source": [
    "# –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—É—é –∫–æ–ª–æ–Ω–∫—É \"labels\"\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(\"labels\")\n",
    "# –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º \"int_labels\" ‚Üí \"labels\"\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"int_labels\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51653f939a5d6971",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T17:36:45.281764Z",
     "start_time": "2025-05-15T17:36:45.243996Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import Value\n",
    "tokenized_dataset = tokenized_dataset.cast_column(\"labels\", Value(\"int64\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed3d7bb00b02141",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T17:36:56.401773Z",
     "start_time": "2025-05-15T17:36:56.384771Z"
    }
   },
   "outputs": [],
   "source": [
    "for example in tokenized_dataset.select(range(5)):\n",
    "    print(type(example[\"labels\"]), example[\"labels\"])  # –î–æ–ª–∂–Ω—ã –±—ã—Ç—å int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba35781d123a527c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T17:46:07.733356Z",
     "start_time": "2025-05-15T17:46:05.453163Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import Value\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è –º–µ—Ç–æ–∫\n",
    "label_to_id = {\n",
    "    'ad hominem': 0,\n",
    "    'ad populum': 1,\n",
    "    'appeal to emotion': 2,\n",
    "    'circular claim': 3,\n",
    "    'deductive fallacy': 4,\n",
    "    'equivocation': 5,\n",
    "    'fallacy of credibility': 6,\n",
    "    'fallacy of extension': 7,\n",
    "    'fallacy of relevance': 8,\n",
    "    'false causality': 9,\n",
    "    'false dilemma': 10,\n",
    "    'faulty generalization': 11,\n",
    "    'intentional': 12,\n",
    "}\n",
    "\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä (–∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –Ω—É–∂–Ω—É—é –º–æ–¥–µ–ª—å)\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# --- –®–∞–≥ 1. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ ---\n",
    "def tokenize_function(example):\n",
    "    # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç: –≤–∫–ª—é—á–∞–µ–º truncation, padding –∏ –∑–∞–¥–∞—ë–º max_length –ø–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –∫–æ –≤—Å–µ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ train_dataset —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω)\n",
    "tokenized_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# --- –®–∞–≥ 2. –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫ ---\n",
    "def convert_labels(example):\n",
    "    # –ï—Å–ª–∏ labels ‚Äî —ç—Ç–æ —Å–ø–∏—Å–æ–∫, –≤–æ–∑—å–º–µ–º –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç\n",
    "    label_value = example[\"labels\"]\n",
    "    if isinstance(label_value, list):\n",
    "        label_value = label_value[0]\n",
    "    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å—Ç—Ä–æ–∫—É –º–µ—Ç–∫–∏ –≤ —á–∏—Å–ª–æ —Å –ø–æ–º–æ—â—å—é —Å–ª–æ–≤–∞—Ä—è label_to_id\n",
    "    return {\"int_labels\": label_to_id[label_value]}\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Å—Ç–æ–ª–±–µ—Ü \"int_labels\" —Å —á–∏—Å–ª–æ–≤—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏\n",
    "tokenized_dataset = tokenized_dataset.map(convert_labels)\n",
    "\n",
    "# --- –®–∞–≥ 3. –£–¥–∞–ª–µ–Ω–∏–µ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞ –∏ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ ---\n",
    "# –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—É—é –∫–æ–ª–æ–Ω–∫—É —Å –º–µ—Ç–∫–∞–º–∏ (—Å—Ç—Ä–æ–∫–æ–≤–æ–π)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(\"labels\")\n",
    "# –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º \"int_labels\" –≤ \"labels\"\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"int_labels\", \"labels\")\n",
    "\n",
    "# --- –®–∞–≥ 4. –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ç–∏–ø–∞ —Å—Ç–æ–ª–±—Ü–∞ ---\n",
    "# –ü—Ä–∏–≤–æ–¥–∏–º –∫–æ–ª–æ–Ω–∫—É \"labels\" –∫ —Ç–∏–ø—É int64\n",
    "tokenized_dataset = tokenized_dataset.cast_column(\"labels\", Value(\"int64\"))\n",
    "\n",
    "# --- –®–∞–≥ 5. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ –¥–ª—è PyTorch ---\n",
    "# –ó–∞–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã –º–æ–¥–µ–ª–∏.\n",
    "columns_to_use = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "# –ï—Å–ª–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç token_type_ids ‚Äî –¥–æ–±–∞–≤–∏–º —Å—é–¥–∞\n",
    "if \"token_type_ids\" in tokenized_dataset.column_names:\n",
    "    columns_to_use.append(\"token_type_ids\")\n",
    "\n",
    "# –û—á–µ–Ω—å –≤–∞–∂–Ω–æ: –≤—ã–∑—ã–≤–∞–µ–º set_format –±–µ–∑ –ø–µ—Ä–µ–ø—Ä–∏—Å–≤–∞–∏–≤–∞–Ω–∏—è!\n",
    "tokenized_dataset.set_format(\"torch\", columns=columns_to_use)\n",
    "\n",
    "# --- –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ö–µ–º—ã ---\n",
    "print(\"Column names:\", tokenized_dataset.column_names)\n",
    "print(\"Features:\", tokenized_dataset.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4328154f397faf75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T18:40:28.476051Z",
     "start_time": "2025-05-15T18:27:35.433114Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1. –ò–º–ø–æ—Ä—Ç—ã\n",
    "from datasets import load_dataset, Value\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# 2. –ó–∞–≥—Ä—É–∑–∫–∞ CSV-—Ñ–∞–π–ª–æ–≤ —Å –¥–∞–Ω–Ω—ã–º–∏\n",
    "data_files = {\n",
    "    \"train\": \"https://huggingface.co/datasets/benmshultz/RuFal_fallacy_detection/resolve/main/fallacies_ru_train.csv\",\n",
    "    \"eval\": \"https://huggingface.co/datasets/benmshultz/RuFal_fallacy_detection/resolve/main/fallacies_ru_test.csv\"\n",
    "}\n",
    "raw_datasets = load_dataset(\"csv\", data_files=data_files)\n",
    "\n",
    "# 3. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è –º–µ—Ç–æ–∫ (label_to_id)\n",
    "label_to_id = {\n",
    "    'ad hominem': 0,\n",
    "    'ad populum': 1,\n",
    "    'appeal to emotion': 2,\n",
    "    'circular claim': 3,\n",
    "    'deductive fallacy': 4,\n",
    "    'equivocation': 5,\n",
    "    'fallacy of credibility': 6,\n",
    "    'fallacy of extension': 7,\n",
    "    'fallacy of relevance': 8,\n",
    "    'false causality': 9,\n",
    "    'false dilemma': 10,\n",
    "    'faulty generalization': 11,\n",
    "    'intentional': 12,\n",
    "}\n",
    "\n",
    "# 4. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ (—Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–π)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
    "\n",
    "# 5. –§—É–Ω–∫—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "# –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π –∏ eval –Ω–∞–±–æ—Ä—ã\n",
    "tokenized_train = raw_datasets[\"train\"].map(tokenize_function, batched=True)\n",
    "tokenized_eval = raw_datasets[\"eval\"].map(tokenize_function, batched=True)\n",
    "\n",
    "# 6. –§—É–Ω–∫—Ü–∏—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –º–µ—Ç–æ–∫ –∏–∑ —Å—Ç—Ä–æ–∫ –≤ —á–∏—Å–ª–∞\n",
    "def convert_labels(example):\n",
    "    label_val = example[\"labels\"]\n",
    "    if isinstance(label_val, list):\n",
    "        label_val = label_val[0]\n",
    "    return {\"int_labels\": label_to_id[label_val]}\n",
    "\n",
    "tokenized_train = tokenized_train.map(convert_labels)\n",
    "tokenized_eval = tokenized_eval.map(convert_labels)\n",
    "\n",
    "# 7. –£–¥–∞–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Å—Ç–æ–ª–±–µ—Ü \"labels\" (—Å—Ç—Ä–æ–∫–æ–≤–æ–≥–æ —Ç–∏–ø–∞) –∏ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –Ω–æ–≤—ã–π —á–∏—Å–ª–æ–≤–æ–π —Å—Ç–æ–ª–±–µ—Ü\n",
    "tokenized_train = tokenized_train.remove_columns(\"labels\")\n",
    "tokenized_train = tokenized_train.rename_column(\"int_labels\", \"labels\")\n",
    "\n",
    "tokenized_eval = tokenized_eval.remove_columns(\"labels\")\n",
    "tokenized_eval = tokenized_eval.rename_column(\"int_labels\", \"labels\")\n",
    "\n",
    "# 8. –ü—Ä–∏–≤–æ–¥–∏–º –∫–æ–ª–æ–Ω–∫—É \"labels\" –∫ —Ç–∏–ø—É int64\n",
    "tokenized_train = tokenized_train.cast_column(\"labels\", Value(\"int64\"))\n",
    "tokenized_eval = tokenized_eval.cast_column(\"labels\", Value(\"int64\"))\n",
    "\n",
    "# 9. –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è PyTorch (–º–µ—Ç–æ–¥ set_format –º—É—Ç–∏—Ä—É–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç \"–Ω–∞ –º–µ—Å—Ç–µ\")\n",
    "columns_to_use = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "if \"token_type_ids\" in tokenized_train.column_names:\n",
    "    columns_to_use.append(\"token_type_ids\")\n",
    "tokenized_train.set_format(\"torch\", columns=columns_to_use)\n",
    "tokenized_eval.set_format(\"torch\", columns=columns_to_use)\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ö–µ–º—ã, –º–æ–∂–Ω–æ –≤—ã–≤–µ—Å—Ç–∏:\n",
    "print(\"Train columns:\", tokenized_train.column_names)\n",
    "print(\"Train features:\", tokenized_train.features)\n",
    "print(\"Eval columns:\", tokenized_eval.column_names)\n",
    "print(\"Eval features:\", tokenized_eval.features)\n",
    "\n",
    "# 10. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"DeepPavlov/rubert-base-cased\",\n",
    "    num_labels=len(label_to_id)\n",
    ")\n",
    "\n",
    "# 11. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è (legacy-–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Å—Ç–∞—Ä—ã—Ö –≤–µ—Ä—Å–∏–π transformers)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_strategy=\"epoch\",      # –î–ª—è –Ω–æ–≤—ã—Ö –≤–µ—Ä—Å–∏–π —ç—Ç–æ evaluation_strategy\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    save_steps=2000,\n",
    "    eval_steps=10,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\"\n",
    ")\n",
    "\n",
    "# 12. –°–æ–∑–¥–∞–µ–º Trainer, –ø–µ—Ä–µ–¥–∞—ë–º –≤ –Ω–µ–≥–æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—É—é –∏ eval –≤—ã–±–æ—Ä–∫–∏\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval\n",
    ")\n",
    "\n",
    "# 13. –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841bca5a88dac726",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T18:42:10.611874Z",
     "start_time": "2025-05-15T18:42:09.442589Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./saved_model\")\n",
    "tokenizer.save_pretrained(\"./saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47661a6277b4dfd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T10:51:26.981313Z",
     "start_time": "2025-05-19T10:07:33.403804Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "\n",
    "# 1. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è –º–µ—Ç–æ–∫ (label_to_id)\n",
    "label_to_id = {\n",
    "    'ad hominem': 0,\n",
    "    'ad populum': 1,\n",
    "    'appeal to emotion': 2,\n",
    "    'circular claim': 3,\n",
    "    'deductive fallacy': 4,\n",
    "    'equivocation': 5,\n",
    "    'fallacy of credibility': 6,\n",
    "    'fallacy of extension': 7,\n",
    "    'fallacy of relevance': 8,\n",
    "    'false causality': 9,\n",
    "    'false dilemma': 10,\n",
    "    'faulty generalization': 11,\n",
    "    'intentional': 12,\n",
    "}\n",
    "\n",
    "# –ò–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è id –≤ –Ω–∞–∑–≤–∞–Ω–∏–µ –ª–æ–≥–∏—á–µ—Å–∫–æ–π –æ—à–∏–±–∫–∏:\n",
    "id_to_label = {v: k for k, v in label_to_id.items()}\n",
    "\n",
    "# 2. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"DeepPavlov/rubert-base-cased\", num_labels=len(label_to_id))\n",
    "model.eval()  # –ø–µ—Ä–µ–≤–æ–¥ –º–æ–¥–µ–ª–∏ –≤ —Ä–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏\n",
    "\n",
    "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (GPU, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 3. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å —Ç–µ–∫—Å—Ç–∞–º–∏\n",
    "df = pd.read_csv(\"cleaned_bertopic_results.csv\")\n",
    "\n",
    "# 4. –ê–≥—Ä–µ–≥–∞—Ü–∏—è softmax‚Äë–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –ø–æ –≤—Å–µ–º –ø–æ—Å—Ç–∞–º\n",
    "num_labels = len(label_to_id)\n",
    "aggregate_probs = np.zeros(num_labels, dtype=np.float64)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for text in df[\"text\"]:\n",
    "        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ (—É—Å–µ—á–µ–Ω–∏–µ, –ø–∞–¥–¥–∏–Ω–≥ –¥–æ max_length=128)\n",
    "        inputs = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏ (GPU –∏–ª–∏ CPU)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits  # —Ä–∞–∑–º–µ—Ä [1, num_labels]\n",
    "        probabilities = torch.softmax(logits, dim=-1).squeeze(0)  # —Ä–∞–∑–º–µ—Ä [num_labels]\n",
    "        aggregate_probs += probabilities.cpu().numpy()\n",
    "\n",
    "# 5. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è: –ø–æ–ª—É—á–∞–µ–º –¥–æ–ª–∏ –æ—Ç —Å—É–º–º—ã\n",
    "total = aggregate_probs.sum()\n",
    "distribution = aggregate_probs / total\n",
    "\n",
    "# 6. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–µ—Ç–æ–∫ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞ (—Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –ø–æ—Ä—è–¥–∫—É id)\n",
    "labels = [id_to_label[i] for i in range(num_labels)]\n",
    "\n",
    "# 7. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=labels, y=distribution, palette=\"viridis\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.xlabel(\"–¢–∏–ø –ª–æ–≥–∏—á–µ—Å–∫–æ–π –æ—à–∏–±–∫–∏\")\n",
    "plt.ylabel(\"–î–æ–ª—è (–Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—É–º–º–∞ softmax –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π)\")\n",
    "plt.title(\"–ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫ –ø–æ –≤—Å–µ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f50f7235efd69ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T11:08:45.969140Z",
     "start_time": "2025-05-19T11:08:45.769252Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º DataFrame —Å fallacy –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–π –¥–æ–ª–µ–π\n",
    "df_distribution = pd.DataFrame({'Fallacy': labels, 'Distribution': distribution})\n",
    "# –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å—Ç–æ–ª–±—Ü—É Distribution –æ—Ç –±–æ–ª—å—à–µ–≥–æ –∫ –º–µ–Ω—å—à–µ–º—É\n",
    "df_distribution = df_distribution.sort_values(by='Distribution', ascending=False)\n",
    "\n",
    "# –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=df_distribution, x='Fallacy', y='Distribution', palette='viridis')\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.xlabel(\"Logical Fallacy\")\n",
    "plt.ylabel(\"Normalized Sum of Softmax Probabilities\")\n",
    "plt.title(\"Aggregated Distribution of Logical Fallacies Across the Dataset\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AI Env)",
   "language": "python",
   "name": "ai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
